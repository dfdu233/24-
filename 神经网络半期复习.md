# 神经网络半期复习



## 1.导论

+ 智能

> 学习 理解 和与环境交互的能力
>
> 人类智能来源于**大脑新皮层**
>
> + 相同的结构与算法
> + 功能层级组织

+ 强/弱人工智能

> 强：像人类一样思考 感知和自我意识 自发学习
>
> 弱：不囊向人类一样推理 可实现特定功能的系统

+ 神经网络与智能产生

> + 神经元基本单位
> + 连接通过学习建立
> + 互通形成神经网络

+ 人工智能发展

> 大数据和计算能力引爆革命

+ 神经网络研究热点与挑战
  + 热点
    + 记忆机制
      + 记忆表达与存储
    + 网络架构
    + 学习算法
      + 最优化理论
    + 应用落地
      + 大语言模型
  + 挑战
    + 安全性脆弱
    + 过分依赖大数据
    + 可解释性不足





## 2.感知机

+ 生物神经网络**抽象**人工神经网络
  + 神经元（神经元模型）
  + 突触连接（连接权）
  + 神经网络（神经网络模型）
  + 学习（学习算法）

> 放电模型 点火率模型



+ 激活函数
  + 硬极限函数
  + 线性函数
  + sigmoid函数



+ 感知机
  + 二分类的线性问题

![image-20241103203637893](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241103203637893.png)



+ 单神经元感知机主要特征
  + 输入向量分为两类
  + 决策边界是线性的



+ 感知机w更新规则

![image-20241105115153289](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241105115153289.png)

![image-20241105115315848](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241105115315848.png)

+ 局限性

线性不可分

噪声敏感

靠近决策平面错判



+ Hamming网络



+ Hopfield网络



## 3.信号与权值空间向量

+ 数据类型
  + 数值型
  + 类别型
  + 图像型
  + 文本型



+ 向量空间

> 满足向量加法和数乘的集合
>
> 包含零向量
>
> 例子：平面，阶数小于等于2的多项式集合，连续函数集合



+ 张成空间

> 一组向量所有线性组合构成的空间



+ 线性相关与无关

> 存在一种线性系数不全为0的线性组合使得结果为0向量则线性相关
>
> 反之要全为0则线性无关



+ 基

> 张成该向量空间的线性无关向量集



+ 向量内积

> 1. 正/负表示两向量方向相同/反
> 2. 内积=a范数乘以b在a向量上投影长度

![image-20241103214239633](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241103214239633.png)

**标准内积**

![image-20241103214332992](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241103214332992.png)

l2**范数**

![image-20241103214446410](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241103214446410.png)

**lp范数**



![image-20241103214517671](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241103214517671.png)

**夹角定义**



### 施密特正交化

> 线性无关向量转换为正交向量

+ 过程

1. 选出第一个向量
2. 得到第k个向量

![image-20241103214752947](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241103214752947.png)



可能问题 哪些线性无关 找正交基



## 4.线性变换

> 定义
>
> ![image-20241103215106886](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241103215106886.png)



+ 形象的例子

![image-20241103215324057](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241103215324057.png)

> 可以看出实际上是变换了基向量
>
> 两个有限维向量空间之间的任意线性变换可以用一个矩阵表示



+ 特征值与特征向量（求法）

![image-20241105014339534](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241105014339534.png)



+ PCA

> 零均值

![image-20241105015409908](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241105015409908.png)

## 5.有监督的Hebb学习

+ 特征值与特征向量

Az=lz

+ 线性联想器

![image-20241105012633641](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241105012633641.png)





![image-20241104230234238](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241104230234238.png)



![image-20241105013536429](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241105013536429.png)



+ 变形
  + 增加学习率参数
  + 增加衰减项目（类似平滑滤波，使最近一次输入更重要）



## 6.神经网络与最优化

+ 学习法则分类
  + 关联学习
  + 性能学习
  + 竞争学习
+ 最优化问题
  + 目标函数
  + 约束条件
  + 参数值
+ 泰勒展开

![image-20241105011118625](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241105011118625.png)



+ 方向导数

> 函数沿指定方向变化率
>
> + 二阶级梯度
> + 强极小点
> + 全局极小点

![image-20241105011433282](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241105011433282.png)





![image-20241104224858659](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241104224858659.png)

> 步骤 
>
> 找到当前梯度
>
> 算出下一个点
>
> 循环



![image-20241104224957334](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241104224957334.png)



## 7.Widrow-Hoff学习

+ 二次函数 梯度与Hessian矩阵

![image-20241105015756090](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241105015756090.png)



+ LMS算法

![image-20241105020301848](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241105020301848.png)

![image-20241105020340990](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241105020340990.png)

+ 稳定性条件

> 学习率小于最大特征值的倒数

![image-20241105020432757](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241105020432757.png)

> 迭代过程
>
> 先算a(0) 即结果
>
> 在算出e(0) 即误差 这里规定的是a(0)与t的差
>
> 再根据w(k+1)=w(k)+2 alpha(学习率) e(k)pT(k)算出w(k+1)
>
> 循环





![image-20241104225400205](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241104225400205.png)



## 8.反向传播

+ J对w的偏导

![image-20241105021852905](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241105021852905.png)

+ J对a(z)的偏导

![image-20241105022044706](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241105022044706.png)





![image-20241104223447395](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241104223447395.png)

![image-20241104223414763](C:\Users\86199\AppData\Roaming\Typora\typora-user-images\image-20241104223414763.png)
